<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-DSL69ECQ36"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-DSL69ECQ36');
  </script>

  <title>Prajwal Gatti</title>
  <meta name="author" content="Prajwal Gatti">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/avatar5.jpeg">
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Prajwal Gatti</name>
              </p>
              <p>
                I am a second year Ph.D. student at the <a href="https://www.bristol.ac.uk/">University of Bristol</a> advised by <a href="https://dimadamen.github.io/">Prof. Dima Damen</a>, in the <a href="https://uob-mavi.github.io/people/">Machine Learning and Computer Vision Group</a>. My research interests are in AI for video understanding.
              </p>
                Prior to this, I was a Research Assistant at the <a href="https://iitj.ac.in/">Indian Institute of Technology Jodhpur</a> advised by <a href="https://scholar.google.co.in/citations?user=vhUg2zIAAAAJ&hl=en" target="_blank">Prof. Anand Mishra</a> where I explored the problems of vision-augemented table-to-text generation, cross-modal image retrieval among other vision-language problems.
              </p>
                I also briefly interned at the Center for Neuroscience, <a href="https://iisc.ac.in/" target="_blank">Indian Institute of Science</a> where I worked on EEG Brain-Computer Interfaces under the advise of <a href="https://scholar.google.com/citations?user=5ij8Y9YAAAAJ&hl=en" target="_blank">Prof. Sridharan Devarajan</a>.
              </p>
                I received my B.E. in Information Science and Engineering at <a href="https://www.dsce.edu.in/" target="_blank">Dayananda Sagar College of Engineering</a> in 2020.
              </p>
              <p style="text-align:center">
                <a href="mailto:prajwal.gatti@bristol.ac.uk">Email</a> &nbsp/&nbsp
                <!-- <a href="resume/Gatti_CV.pdf" target="_blank">CV</a> &nbsp/&nbsp -->
                <a href="https://www.linkedin.com/in/prajwalgatti/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.in/citations?user=TD7aBesAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/prajwalgatti" target="_blank"> GitHub </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img src="images/avatar5.jpeg" width="100%" style="border-radius:0%" class="profile-image">
              
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Feb 2025</b>: Introducing <a href="https://arxiv.org/abs/2502.04144" target="_blank">HD-EPIC</a> – a new richly-annotated egocentric video dataset. Paper and dataset now available!</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Dec 2024</b>: Our work <a href="https://arxiv.org/abs/2412.01987" target="_blank">ShowHowTo</a> is now on Arxiv. Check it out!</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Jul 2024</b>: Attending the <a href="https://iplab.dmi.unict.it/icvss2024/" target="_blank">ICVSS 2024</a> summer school. </li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Jan 2024</b>: Started my Ph.D. in Computer Science at the <a href="https://www.bristol.ac.uk/">University of Bristol</a>, advised by <a href="https://dimadamen.github.io/">Prof. Dima Damen</a>! 🥳</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Dec 2023</b>: Our work <a href="https://vl2g.github.io/projects/cstbir/" ,="" target="_blank">CSTBIR</a> has been accepted at <a href="https://aaai.org/aaai-conference/" ,="" target="_blank"> AAAI 2024</a>.</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Nov 2023</b>: Speaking at the <a href="https://www.aimlsystems.org/2023/">AI-ML 2023</a> (<a href="https://www.aimlsystems.org/2023/ai-india-track/">All India Track</a>) Conference about my work on VisToT.</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Aug 2023</b>: Our work <a href="https://vl2g.github.io/projects/floco/" ,="" target="_blank">Towards Making Flowchart Images Machine Interpretable</a> has been accepted at <a href="https://icdar2023.org/" ,="" target="_blank"> ICDAR 2023</a>.</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Apr 2023</b>: In the organizing team of <a href="https://vl2g.github.io/challenges/wv2023/", target="_blank">Summer Challenge on Writer Verification</a> at <a href="https://events.iitj.ac.in/ncvpripg2023/">NCVPRIPG'23</a>. Consider participating!</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Nov 2022</b>: Will be attending <a href="https://www.aacl2022.org/", target="_blank">AACL 2022</a> virtually, and <a href="https://2022.emnlp.org/", target="_blank">EMNLP 2022</a> in-person at Abu Dhabi.</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Oct 2022</b>: Our work <a href="https://vl2g.github.io/projects/vistot/docs/VISTOT-EMNLP2022.pdf", target="_blank">VisToT</a> has been accepted at <a href="https://2022.emnlp.org/", target="_blank">EMNLP 2022</a>.</li>
                  <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Sep 2022</b>: Our work <a href="https://aclanthology.org/2022.aacl-main.87/", target="_blank">COFAR</a> has been accepted at <a href="https://www.aacl2022.org/", target="_blank">AACL-IJCNLP 2022</a>.</li>
                  <!-- <li style="list-style-position:inside;margin:3px;padding:0;text-align:left;"><b>Mar 2021</b>: Joined the <a href="http://vl2g.github.io/", target="_blank">Vision, Language and Learning Group (VL2G)</a> at IIT Jodhpur as a Research Assistant.</li> -->
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Selected Publications</heading>
              </td>
            </tr>
          </tbody>
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/hdepic-teaser.gif" alt="PontTuset" width="250" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>HD-EPIC: A Highly-Detailed Egocentric Video Dataset</papertitle>
              <br>
              <a href="https://tobyperrett.github.io/">Toby Perrett</a>,
              <a href="https://ahmaddarkhalil.github.io">Ahmad Darkhalil</a>,
              <a href="https://sinhasaptarshi.github.io/">Saptarshi Sinha</a>,
              <a href="https://omar-emara.github.io/">Omar Emara</a>,
              <a href="https://sjpollard.github.io/">Sam Pollard</a>,
              <a href="https://krantiparida.github.io/">Kranti Kumar Parida</a>,
              <a href="">Kaiting Liu</a>,
              <strong>Prajwal Gatti</strong>,
              <a href="https://sid2697.github.io/">Siddhant Bansal</a>,
              <a href="https://keflanagan.github.io/">Kevin Flanagan</a>,
              <a href="https://jacobchalk.github.io/">Jacob Chalk</a>,
              <a href="https://zhifanzhu.github.io/">Zhifan Zhu</a>,
              <a href="">Rhodri Guerrier</a>,
              <a href="">Fahd Abdelazim</a>,
              <a href="https://binzhubz.github.io/">Bin Zhu</a>,
              <a href="https://www.davidemoltisanti.com/research">Davide Moltisanti</a>,
              <a href="https://mwray.github.io/">Michael Wray</a>,
              <a href="https://hazeldoughty.github.io/">Hazel Doughty</a>,
              <a href="https://dimadamen.github.io/">Dima Damen</a>
              <br>
              CVPR 2025
              <br>
              <a href="https://arxiv.org/abs/2502.04144", target="_blank">paper</a> /
              <a href="https://hd-epic.github.io/", target="_blank">project page</a> /
              <a href="https://hd-epic.github.io/index#download", target="_blank">dataset</a> /
              <a href="https://www.youtube.com/watch?v=xxlXweMXKsM", target="_blank">video teaser</a> /
              <a href="https://hd-epic.github.io/demo.html", target="_blank">explore samples</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/showhowto-teaser.png" alt="PontTuset" width="250" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions</papertitle>
              <br>
              <a href="https://soczech.github.io/" target="_blank">Tomáš Souček</a>,
              <strong>Prajwal Gatti</strong>,
              <a href="https://mwray.github.io/" target="_blank">Michael Wray</a>,
              <a href="https://scholar.google.com/citations?user=-9ifK0cAAAAJ" target="_blank">Ivan Laptev</a>,
              <a href="https://dimadamen.github.io/" target="_blank">Dima Damen</a>,
              <a href="https://scholar.google.com/citations?user=NCtKHnQAAAAJ" target="_blank">Josef Sivic</a>
              <br>
              CVPR 2025
              <br>
              <a href="https://arxiv.org/abs/2412.01987", target="_blank">paper</a> /
              <a href="https://soczech.github.io/showhowto/", target="_blank">project page</a> /
              <a href="https://github.com/soCzech/showhowto", target="_blank">code</a> /
              <a href="https://github.com/soCzech/showhowto?tab=readme-ov-file#dataset", target="_blank">dataset</a>
              <p></p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vistransICPR2024.png" alt="PontTuset" width="250" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Show Me the World in My Language: Establishing the First Baseline for Scene-Text to Scene-Text Translation</papertitle>
              <br>
              <a href="" target="_blank">Shreyas Vaidya</a><sup>*</sup>,
              <a href="" target="_blank">Arvind Kumar Sharma</a><sup>*</sup>,
              <strong>Prajwal Gatti</strong>,
              <a href="https://anandmishra22.github.io/" target="_blank">Anand Mishra</a>
              <br>
              ICPR 2024
              <br>
              <a href="https://vl2g.github.io/projects/visTrans/resources/paper.pdf", target="_blank">paper</a> /
              <!-- <a href="https://vl2g.github.io/projects/visTrans/resources/CSTBIR-AAAI24Poster.pdf", target="_blank"></a> / -->
              <a href="https://vl2g.github.io/projects/visTrans/", target="_blank">project page</a>
              <p></p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cstbirAAAI2024.png" alt="PontTuset" width="250" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Composite Sketch+Text Queries for Retrieving Objects with Elusive Names and Complex Interactions</papertitle>
              <br>
              <strong>Prajwal Gatti</strong>,
              <a href="https://www.linkedin.com/in/prajwalgatti/kshitijparikh/" target="_blank">Kshitij Parikh</a>,
              <a href="https://dhritippaul.github.io/" target="_blank">Dhriti Paul</a>,
              <a href="https://sites.google.com/view/manishg/" target="_blank">Manish Gupta</a>,
              <a href="https://anandmishra22.github.io/" target="_blank">Anand Mishra</a>
              <br>
              AAAI 2024
              <br>
              <a href="https://vl2g.github.io/projects/cstbir/resources/paper.pdf", target="_blank">paper</a> /
              <a href="https://vl2g.github.io/projects/cstbir/resources/CSTBIR-AAAI24Poster.pdf", target="_blank">poster</a> /
              <a href="https://vl2g.github.io/projects/cstbir/", target="_blank">project page</a>
              <p></p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/flocoICDAR2023.png" alt="PontTuset" width="250" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>Towards Making Flowchart Images Machine Interpretable</papertitle>
              <br>
              <a href="https://shreyashukla2.github.io/" target="_blank">Shreya Shukla</a>,
              <strong>Prajwal Gatti</strong>,
              <a href="https://yogesh-iitj.github.io/" target="_blank">Yogesh Kumar</a>,
              <a href="https://in.linkedin.com/in/vikash-rs-yadav" target="_blank">Vikash Yadav</a>,
              <a href="https://anandmishra22.github.io/" target="_blank">Anand Mishra</a>
              <br>
              ICDAR 2023
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-41734-4_31", target="_blank">paper</a> /
              <a href="https://vl2g.github.io/projects/floco/docs/FLOCO-ICDAR2023.pdf", target="_blank">pre-print</a> /
              <a href="https://vl2g.github.io/projects/floco/", target="_blank">project page</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vistotEMNLP2022.svg" alt="PontTuset" width="250" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>VisToT: Vision-Augmented Table-to-Text Generation</papertitle>
              <br>
              <strong>Prajwal Gatti</strong>,
              <a href="https://scholar.google.co.in/citations?user=vhUg2zIAAAAJ&hl=en" target="_blank">Anand Mishra</a>,
              <a href="https://scholar.google.co.in/citations?user=eX9PSu0AAAAJ&hl=en" target="_blank">Manish Gupta</a>,
              <a href="https://scholar.google.co.in/citations?user=79PCaM0AAAAJ&hl=en" target="_blank">Mithun Das Gupta</a>
              <br>
              EMNLP 2022
              <br>
              <a href="https://vl2g.github.io/projects/vistot/docs/VISTOT-EMNLP2022.pdf", target="_blank">paper</a> /
              <a href="https://vl2g.github.io/projects/vistot/docs/VisToT-Poster.pdf", target="_blank">poster</a> /
              <a href="https://vl2g.github.io/projects/vistot/", target="_blank">project page</a>
              <p></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cofarAACL2022.png" alt="PontTuset" width="250" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <papertitle>COFAR: Commonsense and Factual Reasoning in Image Search</papertitle>
              <br>
              <strong>Prajwal Gatti</strong>,
              <a href="https://abhiram4572.github.io/" target="_blank">Abhirama Penamakuri</a>,
              <a href="https://scholar.google.co.in/citations?user=-P8PrRsAAAAJ&hl=en" target="_blank">Revant Teotia</a>,
              <a href="https://scholar.google.co.in/citations?user=vhUg2zIAAAAJ&hl=en" target="_blank">Anand Mishra</a>,
              <a href="https://scholar.google.co.in/citations?user=-x6lLvcAAAAJ&hl=en" target="_blank">Shubhashis Sengupta</a>,
              <a href="https://scholar.google.co.in/citations?user=DThywGgAAAAJ&hl=en" target="_blank">Roshni Ramnani</a>
              <br>
              AACL-IJCNLP 2022
              <br>
              <a href="https://aclanthology.org/2022.aacl-main.87/", target="_blank">paper</a> /
              <a href="https://vl2g.github.io/projects/cofar/docs/COFAR-Poster.pdf", target="_blank">poster</a> /
              <a href="https://vl2g.github.io/projects/cofar/", target="_blank">project page</a>
              <p></p>
              <p></p>
              <p>
                <!-- Enabling commonsense and factual reasoning in the text-to-image retrieval using visual named entities.  -->
              </p>
            </td>
          </tr>

        </tbody>
        </table>
      </td>
    </tr>
  </table>
  <p style="text-align:right;font-size:small;">Template credits: <a href="https://jonbarron.info/" target="_blank">this</a> and <a href="https://vjysd.github.io/" target="_blank">this</a></p>
</body>

</html>
